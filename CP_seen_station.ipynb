{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mapie"
      ],
      "metadata": {
        "id": "pGCABMT3Vuyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It6Y-ZTIVokX"
      },
      "outputs": [],
      "source": [
        "# load packages\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score, mean_absolute_percentage_error as mape, mean_squared_error as mse\n",
        "from pyarrow import feather as pq\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "from folium import Marker\n",
        "from shapely import geometry\n",
        "from tqdm import tqdm\n",
        "pd.set_option('display.max_columns', None)\n",
        "from ipywidgets import interact\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV\n",
        "import mapie\n",
        "from mapie import regression\n",
        "from mapie.metrics import regression_coverage_score\n",
        "from mapie.regression import MapieRegressor\n",
        "from mapie.quantile_regression import MapieQuantileRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from mapie.subsample import BlockBootstrap\n",
        "from mapie.time_series_regression import MapieTimeSeriesRegressor\n",
        "#import shap\n",
        "# Feature Importance\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import mean_pinball_loss\n",
        "from sklearn.metrics import make_scorer\n",
        "#conda install tensorflow\n",
        "#from models import regression_model\n",
        "#import data_preprocessing\n",
        "#from conformal_prediction import EnCQR\n",
        "#import utils\n",
        "#import data_loaders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "# load data for monthly performance\n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/Aurora_Thesis/data_converted.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBoc9emKV0aI",
        "outputId": "12283f9d-e160-47b9-d070-03dc82fa3cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT2B22boVokd"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "#data = pd.read_csv(\"data_converted.csv\")\n",
        "data.time = pd.to_datetime(data.time)\n",
        "data = data.reset_index()\n",
        "# need to manually add 2 station Id\n",
        "data.loc[data.station== \"Bologna (BO)\", \"station_id\"] = \"ID1999\"\n",
        "data.loc[data.station== \"San Pietro Capofiume (SPC)\", \"station_id\"] = \"ID1998\"\n",
        "# switch London data\n",
        "data[\"OAtot_2\"] = data.HOA_PMF + data.BBOA_PMF + data.OOAtot_PMF\n",
        "data.loc[data.station == \"London\",\"OAtot_PMF\"] = data.loc[data.station == \"London\",\"OAtot_2\"]\n",
        "# Remove Zurich 2017\n",
        "data = data.loc[(data.station != \"Zurich\") | (data.year != 2017),:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DTkE6keVoke"
      },
      "outputs": [],
      "source": [
        "# CLEAN DATA\n",
        "# remove OA with less than 0.1\n",
        "data= data.loc[data.OAtot_PMF >= 0.1, :]\n",
        "# and stations with less than 30 obs.\n",
        "select = (data.groupby(\"station_id\")[\"OAtot_PMF\"].size() > 30).reset_index()\n",
        "data = data.set_index(\"station_id\")\n",
        "data = data.join(select.set_index(\"station_id\"), rsuffix = \"keep\")\n",
        "data = data.loc[ data.OAtot_PMFkeep == True, :]\n",
        "data = data.reset_index()\n",
        "# add day of week\n",
        "data[\"day_week\"] = data.time.dt.day_of_week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_GU9P8bVoke"
      },
      "outputs": [],
      "source": [
        "# feature engineering\n",
        "data[\"rc_1_1000-rc_1_100\"] = data[\"road_class_1_1000\"] - data[\"road_class_1_100\"]\n",
        "data[\"rc_2_1000-rc_2_100\"] = data[\"road_class_2_1000\"] - data[\"road_class_2_100\"]\n",
        "data[\"rc_3_1000-rc_3_100\"] = data[\"road_class_3_1000\"] - data[\"road_class_3_100\"]\n",
        "# CAMX proportions of components of OA\n",
        "data[\"p_HOA\"] = data[\"HOA_CAMX\"] / data[\"OAtot_CAMX\"]\n",
        "data[\"p_BBOA\"] = data[\"BBOA_CAMX\"] / data[\"OAtot_CAMX\"]\n",
        "data[\"p_OOAtot\"] = data[\"OOAtot_CAMX\"] / data[\"OAtot_CAMX\"]\n",
        "\n",
        "# need to decorralate some land-use variables\n",
        "data[\"diff_agriculture\"] = data[\"agriculture1000\"] - data[\"agriculture500\"]\n",
        "data[\"diff_airports\"] = data[\"airports1000\"] - data[\"airports500\"]\n",
        "data[\"diff_barren\"] = data[\"barren1000\"] - data[\"barren500\"]\n",
        "data[\"diff_industrial\"] = data[\"industrial1000\"] - data[\"industrial500\"]\n",
        "data[\"diff_industrial_transport\"]= data[\"industrial_transport1000\"] - data[\"industrial_transport500\"]\n",
        "data[\"diff_natural_green\"] =  data[\"natural_green1000\"] - data[\"natural_green500\"]\n",
        "data[\"diff_ports\"] = data[\"ports1000\"] - data[\"ports500\"]\n",
        "data[\"diff_roads_rails\"] = data[\"roads_rails1000\"] - data[\"roads_rails500\"]\n",
        "data[\"diff_snow_ice\"] = data[\"snow_ice1000\"] - data[\"snow_ice500\"]\n",
        "data[\"diff_transport\"] = data[\"transport1000\"] - data[\"transport500\"]\n",
        "data[\"diff_urban_fabric\"] = data[\"urban_fabric1000\"] - data[\"urban_fabric500\"]\n",
        "data[\"diff_urban_green\"] = data[\"urban_green1000\"] - data[\"urban_green500\"]\n",
        "data[\"diff_water\"] = data[\"water1000\"] - data[\"water500\"]\n",
        "data[\"diff_wetlands\"] = data[\"wetlands1000\"] - data[\"wetlands500\"]\n",
        "# also for Population and IMD\n",
        "data[\"diff_population\"] = data[\"population_1000\"] - data[\"population_500\"]\n",
        "data[\"diff_imd\"] = data[\"imd1000\"] - data[\"imd500\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEupr5MLVokf",
        "outputId": "56dcecd3-980b-4b23-f6ac-14a0cbfc3b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data (26049, 150)\n",
            "select (25967, 150)\n"
          ]
        }
      ],
      "source": [
        "# eventually need this\n",
        "# stations with at least 100 observations\n",
        "keep = (data.groupby(\"station_id\")[\"OAtot_PMF\"].size() > 100).reset_index()\n",
        "# only stations with at least 100 observations\n",
        "select_data = data.copy(deep=True)\n",
        "\n",
        "for stat_id in data.station_id.unique():\n",
        "    if keep.loc[keep.station_id == stat_id, \"OAtot_PMF\"].iloc[0] == False:\n",
        "        select_data = data.loc[data.station_id != stat_id,:]\n",
        "\n",
        "print(\"data\",data.shape )\n",
        "print(\"select\", select_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxU-aGDmVokg"
      },
      "outputs": [],
      "source": [
        "# Get OA\n",
        "data = data.loc[data.OAtot_PMF.isnull()==False,:]\n",
        "data = data.sort_values(by=\"time\")\n",
        "# Get Y\n",
        "Y = data.loc[:, [\"time\",\"station_id\",\"station\",\"OAtot_PMF\"]]\n",
        "Y = Y.set_index(\"time\")\n",
        "# Get X design\n",
        "# probabaly better the 3 components\n",
        "covars = [\"time\", \"station\", \"station_id\", \"OAtot_PMF\", \"HOA_CAMX\", \"BBOA_CAMX\", \"OOAtot_CAMX\", \"year\", \"month\",\"day_week\",\n",
        "\"temp_CAMX\", \"rh_CAMX\", \"press_CAMX\", \"ws_CAMX\", \"wd_CAMX\", \"pblh_CAMX\", \"wind_x_CAMX\", \"wind_y_CAMX\",\"diff_agriculture\", \"diff_airports\", \"diff_barren\", \"diff_industrial\", \"diff_industrial_transport\",\n",
        "    \"diff_natural_green\", \"diff_ports\", \"diff_roads_rails\", \"diff_snow_ice\", \"diff_transport\", \"diff_urban_fabric\", \"diff_urban_green\", \"diff_water\", \"diff_wetlands\",\n",
        "    \"agriculture500\",\"airports500\", \"barren500\", \"industrial500\", \"industrial_transport500\", \"natural_green500\", \"ports500\", \"roads_rails500\", \"snow_ice500\",\n",
        "    \"transport500\", \"urban_fabric500\",\"urban_green500\", \"water500\", \"wetlands500\",\"diff_imd\",\"imd500\", \"diff_population\", \"population_500\",\"elevation\",\"Lat\",\"Lon\", \"area_grid\",\n",
        "    \"distance_border\", \"distance_mt\"]\n",
        "X = data.loc[: , covars]\n",
        "X = X.set_index(\"time\")\n",
        "X = X.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeG6YvvOVokg",
        "outputId": "f6897198-3923-4c65-cd9f-b564eb68fcb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [00:11<00:00,  8.60it/s]\n"
          ]
        }
      ],
      "source": [
        "# get train/cal/test df\n",
        "# generate train dataset and test dataset\n",
        "X_train = pd.DataFrame(np.zeros(X.shape[1])).T\n",
        "X_cal = pd.DataFrame(np.zeros(X.shape[1])).T\n",
        "X_test = pd.DataFrame(np.zeros(X.shape[1])).T\n",
        "Y_train = pd.Series(0)\n",
        "Y_cal = pd.Series(0)\n",
        "Y_test = pd.Series(0)\n",
        "\n",
        "for station_id in tqdm(data.station_id.unique()):\n",
        "    # get station\n",
        "    X_station = X.loc[X.station_id == station_id, :]\n",
        "    Y_station = Y.loc[Y.station_id == station_id, :]\n",
        "    # train and test split (as in training data proportion)\n",
        "    split = 0.75\n",
        "    index_train = int(np.floor(len(X_station) * split))\n",
        "    Y_train_station = Y_station.iloc[:index_train,:]\n",
        "    Y_test_station = Y_station.iloc[index_train:, :]\n",
        "    X_train_station = X_station.iloc[:index_train,:]\n",
        "    X_test_station = X_station.iloc[index_train:,:]\n",
        "\n",
        "    # additionally split train in half to have a calibration dataset\n",
        "    split = 0.5\n",
        "    index_train = int(np.floor(len(X_train_station) * split))\n",
        "    Y_train_stat = Y_train_station.iloc[:index_train,:]\n",
        "    Y_cal_station = Y_train_station.iloc[index_train:, :]\n",
        "    X_train_stat = X_train_station.iloc[:index_train,:]\n",
        "    X_cal_station = X_train_station.iloc[index_train:,:]\n",
        "\n",
        "    # store in df\n",
        "    X_train = pd.concat((X_train, X_train_stat),axis = 0)\n",
        "    X_cal = pd.concat((X_cal, X_cal_station),axis = 0)\n",
        "    X_test = pd.concat((X_test, X_test_station),axis = 0)\n",
        "    Y_train = pd.concat ((Y_train, Y_train_stat),axis = 0)\n",
        "    Y_cal = pd.concat ((Y_cal, Y_cal_station),axis = 0)\n",
        "    Y_test = pd.concat ((Y_test, Y_test_station),axis = 0)\n",
        "\n",
        "# remove first row/obs. of each df (fix here maybe)\n",
        "X_train = X_train.iloc[1:, 55:]\n",
        "X_train = X_train.fillna(0)\n",
        "X_cal = X_cal.iloc[1:, 55:]\n",
        "X_cal = X_cal.fillna(0)\n",
        "X_test = X_test.iloc[1:, 55:]\n",
        "X_test = X_test.fillna(0)\n",
        "Y_train = Y_train.iloc[1:]\n",
        "Y_cal = Y_cal.iloc[1:]\n",
        "Y_test = Y_test.iloc[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGQdO0EoVokh"
      },
      "outputs": [],
      "source": [
        "# helper functions to compute weighted quantiles\n",
        "def weighted_quantile(values, sample_weight, quantiles,\n",
        "                      values_sorted=False, old_style=False):\n",
        "    \"\"\" Very close to numpy.percentile, but supports weights.\n",
        "    NOTE: quantiles should be in [0, 1]!\n",
        "    :param values: numpy.array with data\n",
        "    :param quantiles: array-like with many quantiles needed\n",
        "    :param sample_weight: array-like of the same length as `array`\n",
        "    :param values_sorted: bool, if True, then will avoid sorting of\n",
        "        initial array\n",
        "    :param old_style: if True, will correct output to be consistent\n",
        "        with numpy.percentile.\n",
        "    :return: numpy.array with computed quantiles.\n",
        "    \"\"\"\n",
        "    values = np.array(values)\n",
        "    quantiles = np.array(quantiles)\n",
        "    if sample_weight is None:\n",
        "        sample_weight = np.ones(len(values))\n",
        "    sample_weight = np.array(sample_weight)\n",
        "    assert np.all(quantiles >= 0) and np.all(quantiles <= 1), \\\n",
        "        'quantiles should be in [0, 1]'\n",
        "\n",
        "    if not values_sorted:\n",
        "        sorter = np.argsort(values)\n",
        "        values = values[sorter]\n",
        "        sample_weight = sample_weight[sorter]\n",
        "\n",
        "    weighted_quantiles = np.cumsum(sample_weight) - 0.5 * sample_weight\n",
        "    if old_style:\n",
        "        # To be convenient with numpy.percentile\n",
        "        weighted_quantiles -= weighted_quantiles[0]\n",
        "        weighted_quantiles /= weighted_quantiles[-1]\n",
        "    else:\n",
        "        weighted_quantiles /= np.sum(sample_weight)\n",
        "    return np.interp(quantiles, weighted_quantiles, values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KlfSb6PVokh"
      },
      "outputs": [],
      "source": [
        "# define weighted Conformal Prediction Sets\n",
        "def weighted_conformal(model, decay_rate, alpha, X_train, Y_train, X_cal, Y_cal, X_test, Y_test):\n",
        "    #model.fit(X_train, Y_train)\n",
        "    y_pred_cal = model.predict(X_cal)\n",
        "    conformity_scores = np.abs(Y_cal - y_pred_cal)\n",
        "    n_cal = len(Y_cal)\n",
        "    t = np.array([n_cal-i for i in range(n_cal)])\n",
        "    # define the fixed weights\n",
        "    weights = decay_rate**((n_cal + 1)-t)\n",
        "    # define the normalized weights\n",
        "    normal_weights = weights/(weights.sum() + 1)\n",
        "    # get the (1-alpha) weighted quantile\n",
        "    q_hat = weighted_quantile(conformity_scores, normal_weights, ((1-alpha) * (1 + 1/n_cal)) )\n",
        "    #plt.figure(figsize=(10,5))\n",
        "    #plt.hist(conformity_scores)\n",
        "    #print(q_hat)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    lower_bound = y_pred_test - q_hat\n",
        "    upper_bound = y_pred_test + q_hat\n",
        "    return y_pred_test, lower_bound, upper_bound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDjyeHYWVoki"
      },
      "outputs": [],
      "source": [
        "def weighted_QCR(model_lower, model_upper, decay_rate, alpha, X_train, Y_train, X_cal, Y_cal, X_test, Y_test):\n",
        "    #model_lower.fit(X_train, Y_train)\n",
        "    #model_upper.fit(X_train, Y_train)\n",
        "    # prediction on calibration set\n",
        "    y_pred_lo = model_lower.predict(X_cal)\n",
        "    y_pred_up = model_upper.predict(X_cal)\n",
        "    # obtain conformity score\n",
        "    conformity_scores = np.zeros(len(Y_cal))\n",
        "    for i in range(len(conformity_scores)):\n",
        "        conformity_scores[i] = np.max((Y_cal[i] - y_pred_up[i], y_pred_lo[i] - Y_cal[i]))\n",
        "\n",
        "    n_cal = len(Y_cal)\n",
        "    t = np.array([n_cal-i for i in range(n_cal)])\n",
        "    # define the fixed weights\n",
        "    weights = decay_rate**((n_cal + 1)-t)\n",
        "    # define the normalized weights\n",
        "    normal_weights = weights/(weights.sum() + 1)\n",
        "    # get the (1-alpha) weighted quantile\n",
        "    q_hat = weighted_quantile(conformity_scores, normal_weights,  ((1-alpha) * (1 + 1/n_cal)))\n",
        "    #plt.figure(figsize=(10,5))\n",
        "    #plt.hist(conformity_scores)\n",
        "    #print(q_hat)\n",
        "    lower_bound = model_lower.predict(X_test) - q_hat\n",
        "    upper_bound = model_upper.predict(X_test) - q_hat\n",
        "    return  lower_bound, upper_bound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhEb3-yQVoki"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "vQpJS3i2Voki",
        "outputId": "3565c5cb-3681-4ccb-e0ab-80264cd7bbfd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(alpha=0.5, loss='quantile', random_state=5)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(alpha=0.5, loss=&#x27;quantile&#x27;, random_state=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(alpha=0.5, loss=&#x27;quantile&#x27;, random_state=5)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# train Boosting on train data, both for mean and for quantiles\n",
        "covars = [\"HOA_CAMX\", \"BBOA_CAMX\", \"OOAtot_CAMX\", \"year\", \"month\",\"day_week\",\n",
        "\"temp_CAMX\", \"rh_CAMX\", \"press_CAMX\", \"ws_CAMX\", \"wd_CAMX\", \"pblh_CAMX\", \"wind_x_CAMX\", \"wind_y_CAMX\",\"diff_agriculture\", \"diff_airports\", \"diff_barren\", \"diff_industrial\", \"diff_industrial_transport\",\n",
        "    \"diff_natural_green\", \"diff_ports\", \"diff_roads_rails\", \"diff_snow_ice\", \"diff_transport\", \"diff_urban_fabric\", \"diff_urban_green\", \"diff_water\", \"diff_wetlands\",\n",
        "    \"agriculture500\",\"airports500\", \"barren500\", \"industrial500\", \"industrial_transport500\", \"natural_green500\", \"ports500\", \"roads_rails500\", \"snow_ice500\",\n",
        "    \"transport500\", \"urban_fabric500\",\"urban_green500\", \"water500\", \"wetlands500\",\"diff_imd\",\"imd500\", \"diff_population\", \"population_500\",\"elevation\",\"Lat\",\"Lon\", \"area_grid\",\n",
        "    \"distance_border\", \"distance_mt\"]\n",
        "alpha = 0.1\n",
        "rf = RandomForestRegressor(random_state=5, n_estimators=500)\n",
        "boost = GradientBoostingRegressor(max_depth = 3, learning_rate= 0.1, random_state=5)\n",
        "boost_median = GradientBoostingRegressor(max_depth = 3, learning_rate= 0.1, random_state=5, loss = \"quantile\", alpha = 0.5)\n",
        "# tune quantiles\n",
        "qlo = GradientBoostingRegressor(loss = \"quantile\", alpha = alpha/2, max_depth = 3, learning_rate= 0.1, random_state=5)\n",
        "qup = GradientBoostingRegressor(loss = \"quantile\", alpha = 1-alpha/2, max_depth = 3, learning_rate= 0.1, random_state=5)\n",
        "mean_pinball_loss_lo = make_scorer(mean_pinball_loss, alpha=alpha/2, greater_is_better=False)\n",
        "mean_pinball_loss_up = make_scorer(mean_pinball_loss, alpha=1- alpha/2, greater_is_better=False)\n",
        "boost_param =  {\"max_depth\":[3,5,7], \"learning_rate\":[0.1,0.01,0.05]}\n",
        "clf = RandomizedSearchCV(qlo, boost_param,n_jobs=-1, scoring = mean_pinball_loss_lo, random_state=50)\n",
        "clf.fit(X_train[covars], Y_train.OAtot_PMF)\n",
        "clf2 = RandomizedSearchCV(qup, boost_param, n_jobs=-1, scoring = mean_pinball_loss_up, random_state=50)\n",
        "clf2.fit(X_train[covars], Y_train.OAtot_PMF)\n",
        "rf.fit(X_train[covars], Y_train.OAtot_PMF)\n",
        "boost.fit(X_train[covars], Y_train.OAtot_PMF)\n",
        "boost_median.fit(X_train[covars], Y_train.OAtot_PMF)\n",
        "#qlo.fit(X_train[covars], Y_train.OAtot_PMF)\n",
        "#qup.fit(X_train[covars], Y_train.OAtot_PMF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqzpvx-xVokj"
      },
      "outputs": [],
      "source": [
        "# train EnbPI\n",
        "n_splits = 10 # how to choose?\n",
        "tscv = TimeSeriesSplit(n_splits = n_splits)\n",
        "rf = RandomForestRegressor(random_state=10)\n",
        "#rf_param = {\"max_features\":[\"sqrt\",\"log2\"]}\n",
        "#clf_forest = GridSearchCV(rf, rf_param, cv = tscv, n_jobs=-1)\n",
        "#clf_forest.fit(X_train[covars], Y_train[\"OAtot_PMF\"])\n",
        "# for ts sampling of data\n",
        "# simple RF\n",
        "rf = RandomForestRegressor(random_state=10)\n",
        "# n_resamplings = B, length = s, agg_function = maybe better quantile?\n",
        "cv_mapiets = BlockBootstrap(n_resamplings=50, length= int(len(Y_train)/2), overlapping=True, random_state=59)\n",
        "# fit EnbPI\n",
        "mapie_enbpi = MapieTimeSeriesRegressor(rf, method = \"enbpi\",\n",
        "                                            cv = cv_mapiets, agg_function = \"mean\", n_jobs = -1)\n",
        "# mapie_enbpi = mapie_enbpi.fit(X_train[covars], Y_train[\"OAtot_PMF\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ3zvkH6Vokk"
      },
      "outputs": [],
      "source": [
        "# conformalize each station\n",
        "# NAIVE/CQR/QUANTILE/WeightNAIVE/WeightCQR/EnbPI\n",
        "def result_conformal(stat_id):\n",
        "    rho = 0.99\n",
        "    # get station data\n",
        "    X_train_stat = X_train.loc[X_train.station_id == stat_id,:]\n",
        "    Y_train_stat = Y_train.loc[X_train.station_id == stat_id, \"OAtot_PMF\"]\n",
        "    X_cal_stat = X_cal.loc[X_cal.station_id == stat_id, covars]\n",
        "    X_test_stat = X_test.loc[X_test.station_id == stat_id, covars]\n",
        "    Y_cal_stat = Y_cal.loc[Y_cal.station_id == stat_id , \"OAtot_PMF\"]\n",
        "    Y_test_stat = Y_test.loc[Y_test.station_id == stat_id , \"OAtot_PMF\"]\n",
        "\n",
        "    # NAIVE\n",
        "    mapie = MapieRegressor(boost, cv = \"prefit\", method = \"base\")\n",
        "    mapie.fit(X_cal_stat, Y_cal_stat)\n",
        "    y_pred_naive, y_pis_naive = mapie.predict(X_test_stat, alpha = alpha)\n",
        "    coverage_naive = regression_coverage_score(Y_test_stat, y_pis_naive[:, 0, 0], y_pis_naive[:, 1, 0])\n",
        "    wid_naive = (y_pis_naive[:, 1, 0] - y_pis_naive[:, 0, 0]).mean()\n",
        "    nor_wid_naive = wid_naive/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    # CQR\n",
        "    mapie = MapieQuantileRegressor(boost_median, cv = \"split\", method = \"quantile\", alpha = alpha)\n",
        "    mapie.fit(X_train[covars], Y_train.OAtot_PMF, X_calib = X_cal_stat, y_calib = Y_cal_stat)\n",
        "    y_pred_cqr, y_pis_cqr = mapie.predict(X_test_stat, alpha = alpha)\n",
        "    # try my own implementation\n",
        "    coverage_cqr = regression_coverage_score(Y_test_stat, y_pis_cqr[:, 0, 0], y_pis_cqr[:, 1, 0])\n",
        "    wid_cqr = (y_pis_cqr[:, 1, 0] - y_pis_cqr[:, 0, 0]).mean()\n",
        "    nor_wid_cqr= wid_cqr/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    # QUANTILE\n",
        "    y_pred_quantile = boost_median.predict(X_test_stat)\n",
        "    y_pis_quantile_l = clf.predict(X_test_stat)\n",
        "    y_pis_quantile_u = clf2.predict(X_test_stat)\n",
        "    coverage_quantile = regression_coverage_score(Y_test_stat, y_pis_quantile_l, y_pis_quantile_u)\n",
        "    wid_quantile = (y_pis_quantile_u - y_pis_quantile_l).mean()\n",
        "    nor_wid_quantile= wid_quantile/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    # weighted NAIVE\n",
        "    y_pred_weighted, y_pis_lower, y_pis_upper = weighted_conformal(boost,  rho, alpha, X_train_stat, Y_train_stat, X_cal_stat, Y_cal_stat, X_test_stat, Y_test_stat)\n",
        "    coverage_weighted = regression_coverage_score(Y_test_stat, y_pis_lower, y_pis_upper)\n",
        "    wid_weighted = (y_pis_upper - y_pis_lower).mean()\n",
        "    nor_wid_weight= wid_weighted/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    # Weighted CQR\n",
        "    y_pis_lowerCQR, y_pis_upperCQR = weighted_QCR(clf, clf2, rho , alpha, X_train_stat, Y_train_stat, X_cal_stat, Y_cal_stat, X_test_stat, Y_test_stat)\n",
        "    coverage_weightedCQR = regression_coverage_score(Y_test_stat, y_pis_lowerCQR, y_pis_upperCQR)\n",
        "    wid_weightedCQR = (y_pis_upperCQR - y_pis_lowerCQR).mean()\n",
        "    nor_wid_weightCQR = wid_weightedCQR/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    # EnbPI\n",
        "    rf = RandomForestRegressor(random_state=10)\n",
        "    # n_resamplings = B, length = s, agg_function = maybe better quantile?\n",
        "    cv_mapiets = BlockBootstrap(n_resamplings=50, length= int(len(Y_train_stat)/2), overlapping=True, random_state=59)\n",
        "    # fit EnbPI\n",
        "    mapie_enbpi = MapieTimeSeriesRegressor(rf, method = \"enbpi\",\n",
        "                                            cv = cv_mapiets, agg_function = \"mean\", n_jobs = -1)\n",
        "    mapie_enbpi = mapie_enbpi.fit(pd.concat((X_train_stat[covars], X_cal_stat[covars]),axis = 0), pd.concat((Y_train_stat, Y_cal_stat),axis = 0))\n",
        "    y_pred_enbpi, y_pis_enbpi = mapie_enbpi.predict(X_test_stat, alpha = alpha, ensemble = True, optimize_beta = False) # # maybe betteer false #true or false\n",
        "    coverage_enbpi = regression_coverage_score(Y_test_stat, y_pis_enbpi[:, 0, 0], y_pis_enbpi[:, 1, 0])\n",
        "    wid_enbpi = (y_pis_enbpi[:, 1, 0] - y_pis_enbpi[:, 0, 0]).mean()\n",
        "    nor_wid_enbpi= wid_enbpi/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    n_test = len(Y_test_stat)\n",
        "\n",
        "\n",
        "    return coverage_naive, nor_wid_naive, coverage_cqr, nor_wid_cqr, coverage_quantile, nor_wid_quantile, coverage_weighted, nor_wid_weight,coverage_weightedCQR, nor_wid_weightCQR, coverage_enbpi, nor_wid_enbpi, n_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVSBko6-Vokl"
      },
      "source": [
        "changing B and S lead to an improvement!\n",
        "try not to optimize beta, did not change anything!\n",
        "then to aggregate differentely\n",
        "\n",
        "\n",
        " then no ensemble, then read about B and s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get results (unweighted)\n",
        "\n",
        "# dict to save results\n",
        "w_naive = {}\n",
        "cov_naive = {}\n",
        "w_cqr = {}\n",
        "cov_cqr = {}\n",
        "w_qr = {}\n",
        "cov_qr = {}\n",
        "w_nw = {}\n",
        "cov_nw = {}\n",
        "w_cqrw = {}\n",
        "cov_cqrw = {}\n",
        "w_enbpi = {}\n",
        "cov_enbpi = {}\n",
        "for stat in tqdm(data.station_id.unique()):\n",
        "  # re-arrange order\n",
        "  cov_naive[stat],w_naive[stat], cov_cqr[stat],w_cqr[stat], cov_qr[stat],w_qr[stat], cov_nw[stat],w_nw[stat], cov_cqrw[stat],w_cqrw[stat], cov_enbpi[stat], w_enbpi[stat], n = result_conformal(stat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-nu0ZiMea9R",
        "outputId": "5f09dff5-497f-4adf-c1a3-e59b51dc8bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [1:05:14<00:00, 38.38s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unweighted mean\n",
        "print(\"avg. width naive: \", np.mean(list(w_naive.values())))\n",
        "print(\"avg. cov naive: \", np.mean(list(cov_naive.values())))\n",
        "print(\"avg. width cqr: \", np.mean(list(w_cqr.values())))\n",
        "print(\"avg. cov cqr: \", np.mean(list(cov_cqr.values())))\n",
        "print(\"avg. width qr: \", np.mean(list(w_qr.values())))\n",
        "print(\"avg. cov qr: \", np.mean(list(cov_qr.values())))\n",
        "print(\"avg. width weigth naive: \", np.mean(list(w_nw.values())))\n",
        "print(\"avg. cov weigth naive: \", np.mean(list(cov_nw.values())))\n",
        "print(\"avg. width weight cqr: \", np.mean(list(w_cqrw.values())))\n",
        "print(\"avg. cov weight cqr: \", np.mean(list(cov_cqrw.values())))\n",
        "print(\"avg. width enbpi: \", np.mean(list(w_enbpi.values())))\n",
        "print(\"avg. cov enbpi: \", np.mean(list(cov_enbpi.values())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHEhCUbufNcR",
        "outputId": "d6078c06-6e59-44f6-cb0b-8d199e91193c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg. width naive:  0.9417962278095121\n",
            "avg. cov naive:  0.827748681995528\n",
            "avg. width cqr:  1.068373279609097\n",
            "avg. cov cqr:  0.863272712269827\n",
            "avg. width qr:  1.1002242506033844\n",
            "avg. cov qr:  0.868193592057597\n",
            "avg. width weigth naive:  1.0453366621668074\n",
            "avg. cov weigth naive:  0.857758187833763\n",
            "avg. width weight cqr:  1.1002242506033844\n",
            "avg. cov weight cqr:  0.8094700564257874\n",
            "avg. width enbpi:  0.491624416310531\n",
            "avg. cov enbpi:  0.6356418413017507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unweighted std dev\n",
        "print(\"std. width naive: \", np.std(list(w_naive.values())))\n",
        "print(\"std. cov naive: \", np.std(list(cov_naive.values())))\n",
        "print(\"std. wid cqr: \", np.std(list(w_cqr.values())))\n",
        "print(\"std. cov cqr: \", np.std(list(cov_cqr.values())))\n",
        "print(\"std. wid qr: \", np.std(list(w_qr.values())))\n",
        "print(\"std. cov qr: \", np.std(list(cov_qr.values())))\n",
        "print(\"std. wid weigth naive: \", np.std(list(w_nw.values())))\n",
        "print(\"std. cov weigth naive: \", np.std(list(cov_nw.values())))\n",
        "print(\"std. wid weight cqr: \", np.std(list(w_cqrw.values())))\n",
        "print(\"std. cov weight cqr: \", np.std(list(cov_cqrw.values())))\n",
        "print(\"std. wid enbpi: \", np.std(list(w_enbpi.values())))\n",
        "print(\"std. cov enbpi: \", np.std(list(cov_enbpi.values())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb1LXbM_fi7f",
        "outputId": "94a20fcb-9928-40b3-b61e-3aa60433599e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "std. width naive:  0.9700161706188613\n",
            "std. cov naive:  0.18550229236284355\n",
            "std. wid cqr:  0.935714323580057\n",
            "std. cov cqr:  0.14344426726205436\n",
            "std. wid qr:  0.9709315156281493\n",
            "std. cov qr:  0.13914491742795934\n",
            "std. wid weigth naive:  1.0513609796551666\n",
            "std. cov weigth naive:  0.1761363894398994\n",
            "std. wid weight cqr:  0.9709315156281492\n",
            "std. cov weight cqr:  0.21489074754858223\n",
            "std. wid enbpi:  0.5942566312846672\n",
            "std. cov enbpi:  0.19474393774147283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# median\n",
        "print(\"median width naive: \", np.median(list(w_naive.values())))\n",
        "print(\"median. cov naive: \", np.median(list(cov_naive.values())))\n",
        "print(\"median. width cqr: \", np.median(list(w_cqr.values())))\n",
        "print(\"median. cov cqr: \", np.median(list(cov_cqr.values())))\n",
        "print(\"median. width qr: \", np.median(list(w_qr.values())))\n",
        "print(\"median. cov qr: \", np.median(list(cov_qr.values())))\n",
        "print(\"median. width weigth naive: \", np.median(list(w_nw.values())))\n",
        "print(\"median. cov weigth naive: \", np.median(list(cov_nw.values())))\n",
        "print(\"median. width weight cqr: \", np.median(list(w_cqrw.values())))\n",
        "print(\"median. cov weight cqr: \", np.median(list(cov_cqrw.values())))\n",
        "print(\"median. width enbpi: \", np.median(list(w_enbpi.values())))\n",
        "print(\"median. cov enbpi: \", np.median(list(cov_enbpi.values())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqzE8S6Rf8uF",
        "outputId": "acc4d8bc-b9ae-42af-cb0c-67a4529c258e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "median width naive:  0.9072481572481572\n",
            "median. cov naive:  0.7283594764207223\n",
            "median. width cqr:  0.9047619047619048\n",
            "median. cov cqr:  0.733069896246856\n",
            "median. width qr:  0.9\n",
            "median. cov qr:  0.6546477470625605\n",
            "median. width weigth naive:  0.9346377306903623\n",
            "median. cov weigth naive:  0.7820729520975818\n",
            "median. width weight cqr:  0.8738636363636363\n",
            "median. cov weight cqr:  0.7550779091395974\n",
            "median. width enbpi:  0.6830429732868757\n",
            "median. cov enbpi:  0.3569237043297161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYqMIBfrVokm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5edfbce-8458-4728-af6a-24ca7b01716b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [1:04:57<00:00, 38.21s/it]\n"
          ]
        }
      ],
      "source": [
        "# GET RESULTS for weighted means ...\n",
        "fcoverage_naive, fnor_wid_naive, fcoverage_cqr, fnor_wid_cqr, fcoverage_quantile, fnor_wid_quantile, fcoverage_weighted, fnor_wid_weight,fcoverage_weightedCQR, fnor_wid_weightCQR, fcoverage_enbpi, fnor_wid_enbpi = 0,0,0,0,0,0,0,0,0,0,0,0\n",
        "n = X.station_id.nunique()\n",
        "N = 0\n",
        "# actually use station_id !\n",
        "# coverage_encqr, nor_wid_encqr,\n",
        "for stat in tqdm(X.station_id.unique()):\n",
        "    coverage_naive, nor_wid_naive, coverage_cqr, nor_wid_cqr, coverage_quantile, nor_wid_quantile, coverage_weighted, nor_wid_weight,coverage_weightedCQR, nor_wid_weightCQR, coverage_enbpi, nor_wid_enbpi, n_test = result_conformal(stat)\n",
        "    fcoverage_naive += coverage_naive*n_test\n",
        "    fnor_wid_naive += nor_wid_naive*n_test\n",
        "    fcoverage_cqr += coverage_cqr*n_test\n",
        "    fnor_wid_cqr += nor_wid_cqr*n_test\n",
        "    fcoverage_quantile += coverage_quantile*n_test\n",
        "    fnor_wid_quantile += nor_wid_quantile*n_test\n",
        "    fcoverage_weighted += coverage_weighted*n_test\n",
        "    fnor_wid_weight += nor_wid_weight*n_test\n",
        "    fcoverage_weightedCQR += coverage_weightedCQR*n_test\n",
        "    fnor_wid_weightCQR += nor_wid_weightCQR*n_test\n",
        "    fcoverage_enbpi += coverage_enbpi*n_test\n",
        "    fnor_wid_enbpi += nor_wid_enbpi*n_test\n",
        "    N += n_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1OF7HuqVokn",
        "outputId": "e8390414-be36-417f-ff80-5aa264a0b68e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coverage naive  0.8755155032839469  normalized width  0.7238463102717966\n",
            "coverage CQR  0.881625171834428  normalized width  0.7681463695042399\n",
            "coverage quantile  0.8843745226821444  normalized width  0.804393993288511\n",
            "coverage weighetd naive  0.8929280586528181  normalized width  0.7858998613290046\n",
            "coverage weighted CQR  0.8524515045058806  normalized width  0.804393993288511\n",
            "coverage enbip  0.6592332365969146  normalized width  0.35872067985208766\n"
          ]
        }
      ],
      "source": [
        "print(\"coverage naive \", fcoverage_naive/N, \" normalized width \",fnor_wid_naive/N)\n",
        "print(\"coverage CQR \", fcoverage_cqr/N, \" normalized width \",fnor_wid_cqr/N)\n",
        "print(\"coverage quantile \", fcoverage_quantile/N, \" normalized width \",fnor_wid_quantile/N)\n",
        "print(\"coverage weighetd naive \", fcoverage_weighted/N, \" normalized width \",fnor_wid_weight/N)\n",
        "print(\"coverage weighted CQR \", fcoverage_weightedCQR/N, \" normalized width \",fnor_wid_weightCQR/N)\n",
        "print(\"coverage enbip \", fcoverage_enbpi/N, \" normalized width \",fnor_wid_enbpi/N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJa5wpCGVokn"
      },
      "outputs": [],
      "source": [
        "# conformalize each station\n",
        "# NAIVE/CQR/QUANTILE/WeightNAIVE/WeightCQR/EnbPI\n",
        "def plot_conformal(stat_id):\n",
        "    rho = 0.99\n",
        "    # get station data\n",
        "    X_train_stat = X_train.loc[X_train.station_id == stat_id,:]\n",
        "    Y_train_stat = Y_train.loc[X_train.station_id == stat_id, \"OAtot_PMF\"]\n",
        "    X_cal_stat = X_cal.loc[X_cal.station_id == stat_id, covars]\n",
        "    X_test_stat = X_test.loc[X_test.station_id == stat_id, covars]\n",
        "    Y_cal_stat = Y_cal.loc[Y_cal.station_id == stat_id , \"OAtot_PMF\"]\n",
        "    Y_test_stat = Y_test.loc[Y_test.station_id == stat_id , \"OAtot_PMF\"]\n",
        "\n",
        "    # NAIVE\n",
        "    mapie = MapieRegressor(boost, cv = \"prefit\", method = \"base\")\n",
        "    mapie.fit(X_cal_stat, Y_cal_stat)\n",
        "    y_pred_naive, y_pis_naive = mapie.predict(X_test_stat, alpha = alpha)\n",
        "    coverage_naive = regression_coverage_score(Y_test_stat, y_pis_naive[:, 0, 0], y_pis_naive[:, 1, 0])\n",
        "    wid_naive = (y_pis_naive[:, 1, 0] - y_pis_naive[:, 0, 0]).mean()\n",
        "    nor_wid_naive = wid_naive/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    # CQR\n",
        "    mapie = MapieQuantileRegressor(boost_median, cv = \"split\", method = \"quantile\", alpha = alpha)\n",
        "    mapie.fit(X_train[covars], Y_train.OAtot_PMF, X_calib = X_cal_stat, y_calib = Y_cal_stat)\n",
        "    y_pred_cqr, y_pis_cqr = mapie.predict(X_test_stat, alpha = alpha)\n",
        "    # try my own implementation\n",
        "    coverage_cqr = regression_coverage_score(Y_test_stat, y_pis_cqr[:, 0, 0], y_pis_cqr[:, 1, 0])\n",
        "    wid_cqr = (y_pis_cqr[:, 1, 0] - y_pis_cqr[:, 0, 0]).mean()\n",
        "    nor_wid_cqr= wid_cqr/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    # QUANTILE\n",
        "    y_pred_quantile = boost_median.predict(X_test_stat)\n",
        "    y_pis_quantile_l = qlo.predict(X_test_stat)\n",
        "    y_pis_quantile_u = qup.predict(X_test_stat)\n",
        "    coverage_quantile = regression_coverage_score(Y_test_stat, y_pis_quantile_l, y_pis_quantile_u)\n",
        "    wid_quantile = (y_pis_quantile_u - y_pis_quantile_l).mean()\n",
        "    nor_wid_quantile= wid_quantile/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    # weighted NAIVE\n",
        "    y_pred_weighted, y_pis_lower, y_pis_upper = weighted_conformal(boost,  rho, alpha, X_train_stat, Y_train_stat, X_cal_stat, Y_cal_stat, X_test_stat, Y_test_stat)\n",
        "    coverage_weighted = regression_coverage_score(Y_test_stat, y_pis_lower, y_pis_upper)\n",
        "    wid_weighted = (y_pis_upper - y_pis_lower).mean()\n",
        "    nor_wid_weight= wid_weighted/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    # Weighted CQR\n",
        "    y_pis_lowerCQR, y_pis_upperCQR = weighted_QCR(clf, clf2, rho , alpha, X_train_stat, Y_train_stat, X_cal_stat, Y_cal_stat, X_test_stat, Y_test_stat)\n",
        "    coverage_weightedCQR = regression_coverage_score(Y_test_stat, y_pis_lowerCQR, y_pis_upperCQR)\n",
        "    wid_weightedCQR = (y_pis_upperCQR - y_pis_lowerCQR).mean()\n",
        "    nor_wid_weightCQR = wid_weightedCQR/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    # EnbPI\n",
        "    # get only station id data here\n",
        "    # get only station id data here\n",
        "    rf = RandomForestRegressor(random_state=10)\n",
        "    # n_resamplings = B, length = s, agg_function = maybe better quantile?\n",
        "    cv_mapiets = BlockBootstrap(n_resamplings=50, length= int(len(Y_train_stat)/2), overlapping=True, random_state=59)\n",
        "    # fit EnbPI\n",
        "    mapie_enbpi = MapieTimeSeriesRegressor(rf, method = \"enbpi\",\n",
        "                                            cv = cv_mapiets, agg_function = \"mean\", n_jobs = -1)\n",
        "    mapie_enbpi = mapie_enbpi.fit(pd.concat((X_train_stat[covars], X_cal_stat[covars]),axis = 0), pd.concat((Y_train_stat, Y_cal_stat),axis = 0))\n",
        "    y_pred_enbpi, y_pis_enbpi = mapie_enbpi.predict(X_test_stat, alpha = alpha, ensemble = True, optimize_beta = False) # # maybe betteer false #true or false\n",
        "    coverage_enbpi = regression_coverage_score(Y_test_stat, y_pis_enbpi[:, 0, 0], y_pis_enbpi[:, 1, 0])\n",
        "    wid_enbpi = (y_pis_enbpi[:, 1, 0] - y_pis_enbpi[:, 0, 0]).mean()\n",
        "    nor_wid_enbpi= wid_enbpi/(Y_test_stat.max() - Y_test_stat.min())\n",
        "\n",
        "    n_test = len(Y_test_stat)\n",
        "\n",
        "    return y_pred_naive, y_pis_naive, y_pred_cqr, y_pis_cqr, y_pis_quantile_l, y_pis_quantile_u, y_pis_lower, y_pis_upper, y_pis_lowerCQR, y_pis_upperCQR, y_pred_enbpi, y_pis_enbpi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOTMQZeQVoko",
        "outputId": "8f82f29a-94c1-4809-b078-0a7107f2d24f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [54:11<00:00, 31.88s/it]\n"
          ]
        }
      ],
      "source": [
        "# get values to plot\n",
        "y_pred = {}\n",
        "y_pis_naive = {}\n",
        "y_pred_cqr = {}\n",
        "y_pis_cqr = {}\n",
        "y_pis_quantile_l = {}\n",
        "y_pis_quantile_u = {}\n",
        "y_pis_lower = {}\n",
        "y_pis_upper = {}\n",
        "y_pis_lowerCQR = {}\n",
        "y_pis_upperCQR = {}\n",
        "y_pred_enbpi = {}\n",
        "y_pis_enbpi = {}\n",
        "\n",
        "for stat in tqdm(X.station_id.unique()):\n",
        "    y_pred[stat], y_pis_naive[stat],y_pred_cqr[stat], y_pis_cqr[stat], y_pis_quantile_l[stat],y_pis_quantile_u[stat], y_pis_lower[stat],y_pis_upper[stat],y_pis_lowerCQR[stat], y_pis_upperCQR[stat], y_pred_enbpi[stat], y_pis_enbpi[stat]  = plot_conformal(stat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri2S7re4Voko"
      },
      "outputs": [],
      "source": [
        "# plot for individual station the comparison\n",
        "def plot_comparison(station_name):\n",
        "    #sns.set_style(\"whitegrid\")\n",
        "    station_id = data.loc[data.station == station_name, \"station_id\"].iloc[0]\n",
        "    y_obs = Y_test.loc[Y_test.station_id == station_id,\"OAtot_PMF\"].reset_index(drop=True)\n",
        "    y_camx = (X_test.loc[Y_test.station_id == station_id,[\"HOA_CAMX\",\"BBOA_CAMX\",\"OOAtot_CAMX\"]].apply(np.sum, axis = 1)).reset_index(drop=True)\n",
        "    fig, ax = plt.subplots(3,2, figsize = (25,15))\n",
        "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor = \"black\")\n",
        "    y_loc = np.max( (y_obs.max(), y_camx.max()) )*0.75\n",
        "    # NAIVE\n",
        "    ax[0,0].plot(y_pred[station_id], label = \"Prediction\", color = \"green\")\n",
        "    ax[0,0].plot(y_obs, label =\"Observed OA\", color = \"blue\")\n",
        "    ax[0,0].plot(y_camx, label =\"CAMx\", color = \"orange\")\n",
        "    ax[0,0].fill_between(y_obs.index, y_pis_naive[station_id][:,0,0], y_pis_naive[station_id][:,1,0], color = \"green\", alpha = 0.2, label = \"Prediction Interval\")\n",
        "    ax[0,0].set_title(\"Standard CP\", fontsize = 20)\n",
        "    ax[0,0].set_ylabel(\"OA\", fontsize = 20)\n",
        "    ax[0,0].set_yticklabels(ax[0,0].get_yticks(),fontsize = 16)\n",
        "    ax[0,0].set_xticklabels(ax[0,0].get_xticks(),fontsize = 16)\n",
        "    naive_coverage = np.round(regression_coverage_score(y_obs, y_pis_naive[station_id][:,0,0], y_pis_naive[station_id][:,1,0]),2)\n",
        "    naive_width = (y_pis_naive[station_id][:,1,0] - y_pis_naive[station_id][:,0,0]).mean()\n",
        "    textstr = \"\\n\".join((\n",
        "        r\"$Coverage=%.2f$\" % (naive_coverage, ),\n",
        "        r\"$Width=%.2f$\" % (naive_width, )))\n",
        "    ax[0,0].text(1,y_loc, textstr, bbox = props,fontsize = 20)\n",
        "    # CQR\n",
        "    ax[0,1].plot(y_pred[station_id], label = \"Prediction\", color = \"green\")\n",
        "    ax[0,1].plot(y_obs, label =\"Observed OA\", color = \"blue\")\n",
        "    ax[0,1].plot(y_camx, label =\"CAMx\", color = \"orange\")\n",
        "    ax[0,1].fill_between(y_obs.index, y_pis_cqr[station_id][:,0,0], y_pis_cqr[station_id][:,1,0], color = \"green\", alpha = 0.2, label = \"Prediction Interval\")\n",
        "    ax[0,1].set_title(\"CQR\", fontsize = 20)\n",
        "    cqr_coverage = np.round(regression_coverage_score(y_obs, y_pis_cqr[station_id][:,0,0], y_pis_cqr[station_id][:,1,0]),2)\n",
        "    cqr_width = (y_pis_cqr[station_id][:,1,0] - y_pis_cqr[station_id][:,0,0]).mean()\n",
        "    textstr = \"\\n\".join((\n",
        "        r\"$Coverage=%.2f$\" % (cqr_coverage, ),\n",
        "        r\"$Width=%.2f$\" % (cqr_width, )))\n",
        "    ax[0,1].text(1,y_loc, textstr, bbox = props, fontsize = 20)\n",
        "    # QUANTILE (ses same prediction as cqr)\n",
        "    ax[1,0].plot(y_pred[station_id], label = \"Prediction\", color = \"green\")\n",
        "    ax[1,0].plot(y_obs, label =\"Observed OA\", color = \"blue\")\n",
        "    ax[1,0].plot(y_camx, label =\"CAMx\", color = \"orange\")\n",
        "    ax[1,0].fill_between(y_obs.index, y_pis_quantile_l[station_id], y_pis_quantile_u[station_id], color = \"green\", alpha = 0.2, label = \"Prediction Interval\")\n",
        "    ax[1,0].set_title(\"Quantile regression\", fontsize = 20)\n",
        "    ax[1,0].set_ylabel(\"OA\", fontsize = 20)\n",
        "    ax[1,0].set_yticklabels(ax[1,0].get_yticks(),fontsize = 16)\n",
        "    ax[1,0].set_xticklabels(ax[1,0].get_xticks(),fontsize = 16)\n",
        "    quantile_coverage = np.round(regression_coverage_score(y_obs, y_pis_quantile_l[station_id], y_pis_quantile_u[station_id]),2)\n",
        "    quantile_width = (y_pis_quantile_u[station_id] - y_pis_quantile_l[station_id]).mean()\n",
        "    textstr = \"\\n\".join((\n",
        "        r\"$Coverage=%.2f$\" % (quantile_coverage, ),\n",
        "        r\"$Width=%.2f$\" % (quantile_width, )))\n",
        "    ax[1,0].text(1,y_loc, textstr, bbox = props, fontsize = 20)\n",
        "    # WEIGHETD NAIVE (same prediction as naive)\n",
        "    ax[1,1].plot(y_pred[station_id], label = \"Prediction\",color = \"green\")\n",
        "    ax[1,1].plot(y_obs, label =\"Observed OA\",color = \"blue\")\n",
        "    ax[1,1].plot(y_camx, label =\"CAMx\", color = \"orange\")\n",
        "    ax[1,1].fill_between(y_obs.index, y_pis_lower[station_id], y_pis_upper[station_id], color = \"green\", alpha = 0.2, label = \"Prediction Interval\")\n",
        "    ax[1,1].set_title(\"Weighted Standard CP\", fontsize = 20)\n",
        "    wnaive_coverage = np.round(regression_coverage_score(y_obs, y_pis_lower[station_id], y_pis_upper[station_id]),2)\n",
        "    wnaive_width = (y_pis_upper[station_id] - y_pis_lower[station_id]).mean()\n",
        "    textstr = \"\\n\".join((\n",
        "        r\"$Coverage=%.2f$\" % (wnaive_coverage, ),\n",
        "        r\"$Width=%.2f$\" % (wnaive_width)))\n",
        "    ax[1,1].text(1,y_loc, textstr, bbox = props, fontsize = 20)\n",
        "    # WEIGHTED CQR (same prediction as cqr)\n",
        "    ax[2,0].plot(y_pred[station_id], label = \"Prediction\",color =\"green\")\n",
        "    ax[2,0].plot(y_obs, label =\"Observed OA\", color = \"blue\")\n",
        "    ax[2,0].plot(y_camx, label =\"CAMx\",color = \"orange\")\n",
        "    ax[2,0].fill_between(y_obs.index, y_pis_lowerCQR[station_id], y_pis_upperCQR[station_id], color = \"green\", alpha = 0.2, label = \"Prediction Interval\")\n",
        "    ax[2,0].set_title(\"Weighted CQR\", fontsize = 20)\n",
        "    ax[2,0].set_ylabel(\"OA\", fontsize = 20)\n",
        "    ax[2,0].set_xticklabels(ax[2,0].get_xticks(),fontsize = 16)\n",
        "    ax[2,0].set_yticklabels(ax[2,0].get_yticks(),fontsize = 16)\n",
        "    wcqr_coverage = np.round(regression_coverage_score(y_obs, y_pis_lowerCQR[station_id], y_pis_upperCQR[station_id]),2)\n",
        "    wcqr_width = (y_pis_upperCQR[station_id] - y_pis_lowerCQR[station_id]).mean()\n",
        "    textstr = \"\\n\".join((\n",
        "        r\"$Coverage=%.2f$\" % (wcqr_coverage, ),\n",
        "        r\"$Width=%.2f$\" % (wcqr_width, )))\n",
        "    ax[2,0].text(1,y_loc, textstr, bbox = props, fontsize = 20)\n",
        "    # ENBPI\n",
        "    ax[2,1].plot(y_pred_enbpi[station_id], label = \"Prediction\", color = \"green\")\n",
        "    ax[2,1].plot(y_obs, label =\"Observed OA\",color = \"blue\")\n",
        "    ax[2,1].plot(y_camx, label =\"CAMx\",color = \"orange\")\n",
        "    ax[2,1].fill_between(y_obs.index, y_pis_enbpi[station_id][:,0,0], y_pis_enbpi[station_id][:,1,0], color = \"green\", alpha = 0.2, label = \"Prediction Interval\")\n",
        "    ax[2,1].set_title(\"EnbPI\", fontsize = 20)\n",
        "    ax[2,1].set_yticklabels(ax[2,1].get_yticks(),fontsize = 16)\n",
        "    ax[2,1].set_xticklabels(ax[2,1].get_xticks(),fontsize = 16)\n",
        "    enbpi_coverage = np.round(regression_coverage_score(y_obs, y_pis_enbpi[station_id][:,0,0], y_pis_enbpi[station_id][:,1,0]),2)\n",
        "    enbpi_width = (y_pis_enbpi[station_id][:,1,0] - y_pis_enbpi[station_id][:,0,0]).mean()\n",
        "    textstr = \"\\n\".join((\n",
        "        r\"$Coverage=%.2f$\" % (enbpi_coverage, ),\n",
        "        r\"$Width=%.2f$\" % (enbpi_width, )))\n",
        "    ax[2,1].text(1,y_loc, textstr, bbox = props, fontsize = 20)\n",
        "    plt.legend(loc='upper center', bbox_to_anchor=(-0.1, -0.15),\n",
        "                    fancybox=True, shadow=True, ncol=5, fontsize = 20)\n",
        "\n",
        "\n",
        "    fig.suptitle(station_id + \": \" + station_name, fontsize = 30)\n",
        "    for ax in ax.flat:\n",
        "        ax.label_outer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interact(plot_comparison, station_name = data.station.unique())"
      ],
      "metadata": {
        "id": "pJPi3g8Yhatk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvgCpdtFVokp"
      },
      "outputs": [],
      "source": [
        "from matplotlib.offsetbox import AnnotationBbox, TextArea\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "from scipy.stats import randint, uniform\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
        "from mapie.metrics import (regression_coverage_score,\n",
        "                           regression_mean_width_score)\n",
        "from mapie.subsample import Subsample\n",
        "random_state = 23\n",
        "rng = np.random.default_rng(random_state)\n",
        "round_to = 3\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAGVqEvJVokq"
      },
      "outputs": [],
      "source": [
        "# plot all methods x vs y randomly getting stations\n",
        "def sort_y_values(y_test, y_pred, y_pis):\n",
        "    \"\"\"\n",
        "    Sorting the dataset in order to make plots using the fill_between function.\n",
        "    \"\"\"\n",
        "    indices = np.argsort(y_test)\n",
        "    y_test_sorted = np.array(y_test)[indices]\n",
        "    y_pred_sorted = y_pred[indices]\n",
        "    y_lower_bound = y_pis[:, 0, 0][indices]\n",
        "    y_upper_bound = y_pis[:, 1, 0][indices]\n",
        "    return y_test_sorted, y_pred_sorted, y_lower_bound, y_upper_bound\n",
        "\n",
        "def sort_y_values2(y_test, y_pred, y_lower, y_upper):\n",
        "    \"\"\"\n",
        "    Sorting the dataset in order to make plots using the fill_between function.\n",
        "    \"\"\"\n",
        "    indices = np.argsort(y_test)\n",
        "    y_test_sorted = np.array(y_test)[indices]\n",
        "    y_pred_sorted = y_pred[indices]\n",
        "    y_lower_bound = y_lower[indices]\n",
        "    y_upper_bound = y_upper[indices]\n",
        "    return y_test_sorted, y_pred_sorted, y_lower_bound, y_upper_bound\n",
        "\n",
        "\n",
        "def plot_prediction_intervals(\n",
        "    title,\n",
        "    axs,\n",
        "    y_test_sorted,\n",
        "    y_pred_sorted,\n",
        "    lower_bound,\n",
        "    upper_bound,\n",
        "    coverage,\n",
        "    width,\n",
        "    num_plots_idx\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot of the prediction intervals for each different conformal\n",
        "    method.\n",
        "    \"\"\"\n",
        "\n",
        "    axs.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "    axs.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "\n",
        "    lower_bound_ = np.take(lower_bound, num_plots_idx)\n",
        "    y_pred_sorted_ = np.take(y_pred_sorted, num_plots_idx)\n",
        "    y_test_sorted_ = np.take(y_test_sorted, num_plots_idx)\n",
        "\n",
        "    error = np.abs(y_pred_sorted_-lower_bound_)\n",
        "\n",
        "    warning1 = y_test_sorted_ > y_pred_sorted_+error\n",
        "    warning2 = y_test_sorted_ < y_pred_sorted_-error\n",
        "    warnings = warning1 + warning2\n",
        "    axs.errorbar(\n",
        "        y_test_sorted_[~warnings],\n",
        "        y_pred_sorted_[~warnings],\n",
        "        yerr=error[~warnings],\n",
        "        capsize=5, marker=\"o\", elinewidth=2, linewidth=0, color = \"blue\",\n",
        "        label=\"Inside prediction interval\"\n",
        "        )\n",
        "    axs.errorbar(\n",
        "        y_test_sorted_[warnings],\n",
        "        y_pred_sorted_[warnings],\n",
        "        yerr=error[warnings],\n",
        "        capsize=5, marker=\"o\", elinewidth=2, linewidth=0, color=\"red\",\n",
        "        label=\"Outside prediction interval\"\n",
        "        )\n",
        "    axs.scatter(\n",
        "        y_test_sorted_[warnings],\n",
        "        y_test_sorted_[warnings],\n",
        "        marker=\"*\", color=\"green\",\n",
        "        label=\"True value\"\n",
        "    )\n",
        "    axs.set_xlabel(\"Observed OA\")\n",
        "    axs.set_ylabel(\"Predicted OA\")\n",
        "    #ab = AnnotationBbox(TextArea(\n",
        "           # f\"Coverage: {np.round(coverage, round_to)}\\n\"\n",
        "          #  + f\"Interval width: {np.round(width, round_to)}\"),xy=(np.min(y_test_sorted_)*3, np.max(y_pred_sorted_+error)*0.95),)\n",
        "\n",
        "    lims = [\n",
        "        np.min([axs.get_xlim(), axs.get_ylim()]),  # min of both axes\n",
        "        np.max([axs.get_xlim(), axs.get_ylim()]),  # max of both axes\n",
        "    ]\n",
        "    pt = (0, 0)\n",
        "    axs.axline(pt, slope=1, color='black', label = \"x=y\")\n",
        "    #axs.plot(0,1, '--', alpha=0.75, color=\"black\", label=\"x=y\")\n",
        "    #axs.add_artist(ab)\n",
        "    axs.set_title(title, fontweight='bold', fontsize = 20)\n",
        "    axs.set_ylabel(axs.get_ylabel(), fontsize = 18)\n",
        "    axs.set_xlabel(axs.get_xlabel(), fontsize = 18)\n",
        "    # change the fontsize\n",
        "    axs.tick_params(axis='x', labelsize=16)\n",
        "    axs.tick_params(axis='y', labelsize=16)\n",
        "    #fig.supxlabel('common_x')\n",
        "    #fig.supylabel('common_y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anPoXTmWVokq"
      },
      "outputs": [],
      "source": [
        "# multiple stations plot\n",
        "fig, ax = plt.subplots(3,2, figsize = (20,12), sharey = True)\n",
        "sns.set_style(\"whitegrid\")\n",
        "axs = [ ax[0,0], ax[0,1], ax[1,0], ax[1,1], ax[2,0], ax[2,1]]\n",
        "strategy = [\"Standard CP\", \"CQR\", \"Quantile Regression\", \"Weighted CP\", \"Weighted CQR\", \"EnbPI\"]\n",
        "perc_obs_plot = 0.05\n",
        "# plot only they have at least 10 data points in the test set\n",
        "keep = (Y_test.groupby(\"station_id\")[\"OAtot_PMF\"].size() > 10).reset_index()\n",
        "Y_test2 = Y_test.set_index(\"station_id\").join(keep.set_index(\"station_id\"), rsuffix = \"pr\")\n",
        "Y_test2 = Y_test2.loc[Y_test2.OAtot_PMFpr == True,:]\n",
        "for idx, stat_id in enumerate(Y_test2.reset_index().station_id.unique()):\n",
        "    y_test = Y_test.loc[Y_test.station_id == stat_id, \"OAtot_PMF\"].reset_index(drop=True)\n",
        "    num_plots = rng.choice(len(y_test), int(perc_obs_plot*len(y_test)), replace=False)\n",
        "    y_test_sorted = {}\n",
        "    y_pred_sorted = {}\n",
        "    y_lower_sorted = {}\n",
        "    y_upper_sorted = {}\n",
        "    y_test_sorted[\"Standard CP\"] , y_pred_sorted[\"Standard CP\"], y_lower_sorted[\"Standard CP\"], y_upper_sorted[\"Standard CP\"] = sort_y_values(y_test, y_pred[stat_id], y_pis_naive[stat_id])\n",
        "    y_test_sorted[\"CQR\"] , y_pred_sorted[\"CQR\"], y_lower_sorted[\"CQR\"], y_upper_sorted[\"CQR\"]= sort_y_values(y_test, y_pred[stat_id], y_pis_cqr[stat_id])\n",
        "    y_test_sorted[\"Quantile Regression\"], y_pred_sorted[\"Quantile Regression\"], y_lower_sorted[\"Quantile Regression\"], y_upper_sorted[\"Quantile Regression\"] = sort_y_values2(y_test,y_pred[stat_id], y_pis_quantile_l[stat_id], y_pis_quantile_u[stat_id])\n",
        "    y_test_sorted[\"Weighted CP\"] , y_pred_sorted[\"Weighted CP\"], y_lower_sorted[\"Weighted CP\"], y_upper_sorted[\"Weighted CP\"] = sort_y_values2(y_test, y_pred[stat_id], y_pis_lower[stat_id], y_pis_upper[stat_id])\n",
        "    y_test_sorted[\"Weighted CQR\"] , y_pred_sorted[\"Weighted CQR\"], y_lower_sorted[\"Weighted CQR\"], y_upper_sorted[\"Weighted CQR\"] = sort_y_values2(y_test, y_pred[stat_id], y_pis_lowerCQR[stat_id], y_pis_upperCQR[stat_id])\n",
        "    y_test_sorted[\"EnbPI\"] , y_pred_sorted[\"EnbPI\"], y_lower_sorted[\"EnbPI\"], y_upper_sorted[\"EnbPI\"] = sort_y_values(y_test, y_pred_enbpi[stat_id], y_pis_enbpi[stat_id])\n",
        "\n",
        "\n",
        "    for idx, (axis, strat) in enumerate(zip(axs, strategy)):\n",
        "        plot_prediction_intervals(\n",
        "                strat,\n",
        "                axis,\n",
        "                y_test_sorted[strat],\n",
        "                y_pred_sorted[strat],\n",
        "                y_lower_sorted[strat],\n",
        "                y_upper_sorted[strat],\n",
        "                10,\n",
        "                2,\n",
        "                10 # num_plots\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "lines_labels = [axis.get_legend_handles_labels() for ax in fig.axes]\n",
        "lines, labels = [sum(_, []) for _ in zip(*lines_labels)]\n",
        "plt.legend(\n",
        "                      lines[:2] + lines[-2:], labels[:2] + labels[-2:],\n",
        "                      loc='upper center',\n",
        "                      bbox_to_anchor=(0, -0.225),\n",
        "                      fancybox=True,\n",
        "                      shadow=True,\n",
        "                      ncol=2, fontsize = 20\n",
        "                  )\n",
        "for ax in ax.flat:\n",
        "        ax.label_outer()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWEOPzKyVokq"
      },
      "outputs": [],
      "source": [
        " def plot_stat(station_name):\n",
        "    stat_id = data.loc[data.station == station_name, \"station_id\"].iloc[0]\n",
        "    y_test = Y_test.loc[Y_test.station_id == stat_id, \"OAtot_PMF\"].reset_index(drop=True)\n",
        "    y_test_sorted = {}\n",
        "    y_pred_sorted = {}\n",
        "    y_lower_sorted = {}\n",
        "    y_upper_sorted = {}\n",
        "    y_test_sorted[\"Standard CP\"] , y_pred_sorted[\"Standard CP\"], y_lower_sorted[\"Standard CP\"], y_upper_sorted[\"Standard CP\"] = sort_y_values(y_test, y_pred[stat_id], y_pis_naive[stat_id])\n",
        "    y_test_sorted[\"CQR\"] , y_pred_sorted[\"CQR\"], y_lower_sorted[\"CQR\"], y_upper_sorted[\"CQR\"]= sort_y_values(y_test, y_pred[stat_id], y_pis_cqr[stat_id])\n",
        "    y_test_sorted[\"Quantile Regression\"], y_pred_sorted[\"Quantile Regression\"], y_lower_sorted[\"Quantile Regression\"], y_upper_sorted[\"Quantile Regression\"] = sort_y_values2(y_test,y_pred[stat_id], y_pis_quantile_l[stat_id], y_pis_quantile_u[stat_id])\n",
        "    y_test_sorted[\"Weighted CP\"] , y_pred_sorted[\"Weighted CP\"], y_lower_sorted[\"Weighted CP\"], y_upper_sorted[\"Weighted CP\"] = sort_y_values2(y_test, y_pred[stat_id], y_pis_lower[stat_id], y_pis_upper[stat_id])\n",
        "    y_test_sorted[\"Weighted CQR\"] , y_pred_sorted[\"Weighted CQR\"], y_lower_sorted[\"Weighted CQR\"], y_upper_sorted[\"Weighted CQR\"] = sort_y_values2(y_test, y_pred[stat_id], y_pis_lowerCQR[stat_id], y_pis_upperCQR[stat_id])\n",
        "    y_test_sorted[\"EnbPI\"] , y_pred_sorted[\"EnbPI\"], y_lower_sorted[\"EnbPI\"], y_upper_sorted[\"EnbPI\"] = sort_y_values(y_test, y_pred_enbpi[stat_id], y_pis_enbpi[stat_id])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(3,2, figsize = (24,12), sharey = True)\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    axs = [ ax[0,0], ax[0,1], ax[1,0], ax[1,1], ax[2,0], ax[2,1]]\n",
        "    strategy = [\"Standard CP\", \"CQR\", \"Quantile Regression\", \"Weighted CP\", \"Weighted CQR\",\"EnbPI\"]\n",
        "    perc_obs_plot = 1\n",
        "    num_plots = rng.choice(len(y_test), int(perc_obs_plot*len(y_test)), replace=False)\n",
        "    for axis, strat in zip(axs, strategy):\n",
        "        plot_prediction_intervals(\n",
        "                    strat,\n",
        "                    axis,\n",
        "                    y_test_sorted[strat],\n",
        "                    y_pred_sorted[strat],\n",
        "                    y_lower_sorted[strat],\n",
        "                    y_upper_sorted[strat],\n",
        "                    10,\n",
        "                    2,\n",
        "                    num_plots\n",
        "                    )\n",
        "\n",
        "    lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes]\n",
        "    lines, labels = [sum(_, []) for _ in zip(*lines_labels)]\n",
        "    plt.legend(\n",
        "                lines[:4], labels[:4],\n",
        "                loc='upper center',\n",
        "                bbox_to_anchor=(0, -0.225),\n",
        "                fancybox=True,\n",
        "                shadow=True,\n",
        "                ncol=2, fontsize = 20\n",
        "            )\n",
        "    for ax in ax.flat:\n",
        "        ax.label_outer()\n",
        "    plt.suptitle(station_name, fontsize = 30)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JISb5S0HVokr"
      },
      "outputs": [],
      "source": [
        "interact(plot_stat, station_name = Y_test.station.unique())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}